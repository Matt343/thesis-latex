% TEMPLATE for Usenix papers, specifically to meet requirements of
% USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
% written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
% Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
% both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

\documentclass[letterpaper,twocolumn,11pt]{article}
\usepackage{epsfig,endnotes,amsmath,pgf,hyperref,mathpartir,authblk,comment}
\usepackage[toc,page]{appendix}

\begin{document}

% don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Parametric Polymorphism in the Go Programming Language}

\author[1]{Matthew Allen}
\author[1,2]{Jan S. Rellermeyer}
\affil[1]{University of Texas at Austin}
\affil[2]{IBM Research}
\renewcommand\Authands{ and }

% \author{
%   {\rm Matthew Allen}\\
%   University of Texas
%   \and
%   {\rm Jan S. Rellermeyer}
% }

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}
An extension to the Go language was developed that introduces parametric polymorphism in the form of generic functions. The changes to the language and the compiler needed to implement the type system extensions are discussed, and alternative implementation strategies are described. The resulting implementation of generic functions is backwards compatible with the existing Go standard and is consistent with the design goals of the language. The overhead of the current prototype implementation is assessed based on two commonly used programming patterns. 

\section{Introduction} \label{introduction}

Statically checked, typed-safe languages offer the programmer the ability to reason about the kinds of data that a program operates on and ensure that only valid data is used to perform calculations. When using a language with a static type system, there are guarantees of correctness that are offered, and type errors can be detected at compile time rather than at run time \cite{milner1978theory}. As with any form of static analysis, compile time type checking cannot account for all runtime behavior of the program, and so it must be conservative in rejecting potentially valid programs to ensure that type errors cannot occur. If a language implements a limited static type system without support for high level abstraction, the restrictions of the type system can prevent the programmer from writing abstract or general code. In this case, the type system inhibits programmers who use higher level abstraction, rather than assisting them. By adding more power to the type system, the language can become more expressive and the type system can become less of a burden on the programmer. Parametric polymorphism is a common feature of static type systems that allows functions or data structures to be written generically, so that they can operate on values of many types while still maintaining static type safety \cite{cardelli1987basic}. Generic functions and data structures have type parameters which are substituted for specific types when the function or data structure is used. These type parameters may be bounded, so that only a certain class of types can be used for a type parameter, which allows the generic construct to use some of the common features of its parameterized types.

\section{Motivation} \label{motivation}

The Go programming language features a static type system, but the semantics of the type system are limited \cite{gospec}. Without a way to use parametric polymorphism, it is impossible to write statically type checked functions that can be reused for multiple similar types. This leads programmers to either duplicate code many times to create separate, specialized versions of otherwise identical functions, or to bypass the static type system using casting and reflection \cite{gofaq}.  This paper outlines an extension to the Go language that includes generic functions that are statically checked and transformed into standard Go code at compile time.

The design goals for the Go language include clear semantics and ease of use, as well as runtime performance and concurrency support \cite{godesign}. The proposed extension to the language introduces additional complexity to the type system, but given the prevalence of parametric polymorphism in statically typed languages and the relative simplicity of this generic system, the type system is still understandable enough to satisfy the ease of use goal. The generic function implementation is also backwards compatible with existing Go syntax and semantics, so it is not necessary to update existing code to use the generic system. Runtime performance is an important consideration for the language extension, and is discussed in Section \ref{results}. The goal of this initial implementation is to serve as a proof of concept for the proposed type system extension, so performance was a secondary concern for this implementation.

Since Go is a compiled language, changes to the language specification must be accomplished by changing the compiler(s) that implement the language. The new language constructs necessary to implement generic functions require changes to the grammar that defines valid Go programs. Because of this, the parser for the language must be updated, and new node types must be added to the abstract syntax tree (AST) of the language. These changes to the parser are outlined in section \ref{parsing}. Once the compiler can parse the new language constructs, the type checking system must be updated with the new type rules. These changes to the type checking system in the compiler are detailed in section \ref{type_checking}. Section \ref{code_generation} explains the changes needed to generate generic code correctly, as well as the trade offs for different code generation strategies. 

\section{Compiler Architecture} \label{architecture}

There are several compiler implementations for the Go language, each with different advantages and drawbacks. The main compiler is referred to as the Go Compiler, or GC, and is the original version of the compiler. Originally written in C, GC was made self-hosting in Go version 1.5, and it is now written almost entirely in Go \cite{goselfhost}. Unfortunately, the process for transitioning GC from C to Go was mostly automated, and as a result the codebase has not truly made use of the features or advantages that Go has over C. One consequence of this is that GC uses hand-crafted parsing and type checking code derived from the C version despite the existence of parsing and type checking packages in the Go standard library. The code for performing these tasks is relatively tightly coupled with other parts of the system. For this reason, adding new features to the GC system is difficult, and a significant amount of work would have to be dedicated to working around the complexities of the C-like software design.

 Another major compiler implementation is GCCGo, which is a Go frontend for the GNU Compiler Collection (GCC) \cite{gcc}. GCCGo is written in C++ in the style of the GCC codebase, and it does not make use of the parsing and type checking libraries from the Go standard library. As a GCC frontend, the compiler inherits much of the complexity that is required of such a flexible compiler, and modifications to it must account for this overhead.

A less commonly used community compiler, LLGO, was chosen to be modified. LLGO is a frontend for the LLVM compiler system \cite{llvm} that is written in Go, and it uses the standard library parsing and type checking packages. Because it uses these standard components, it was the easiest system to add modifications to and extend. Since the goal of this project was to provide a proof of concept for a generic type system in Go, a choice was made to sacrifice widespread usage in favor of ease of development and prototyping speed. LLGO implements all of the features of the Go specification, and uses the same Go runtime as GCCGo.

The LLGO compiler consists of the implementation of the Go runtime and a frontend compiler that emits LLVM intermediate representation (IR). The main program emulates the command line API of GC and GCCGo, providing support for resolving dependencies, building packages, and linking programs. When a program is processed by the compiler, it first invokes the parser on the input files to generate a canonical AST for the program. Next, the type checker is invoked on the AST, which generates type information for each expression and definition and ensures that all of the rules of the type system are followed. Once the type information has been generated, the AST is transformed into a single static assignment (SSA) \cite{ssa} form, which makes the AST more compatible with the SSA form of the LLVM IR. At this stage the LLVM specific translation occurs, generating IR from the SSA form of the program that can be fed into the LLVM system. Finally, the LLVM system outputs an executable, binary version of the input program. This architecture is designed to support a high level of modularity and separation of the various parts of the compilation process. The modularity of the system makes it much easier to implement new features in discrete parts, and provide ways to test the individual parts of the system in isolation.

\section{Parsing} \label{parsing}

The first change that was required to implement generic functions was to extend the parser to support the new language constructs. The Go standard library contains a module, \texttt{go/parser}, that implements a recursive descent parser for the Go language grammar. The language grammar was designed to be an LL(1) grammar, which means it can be parsed by a recursive descent parser with single element lookahead and no backtracking in linear time. For reasons discussed below, a naive grammar for generic functions would introduce ambiguity, and would require a backtracking parser to implement. To avoid an extensive redesign of the existing, non-backtracking, parser, the extensions to the language were constrained to keep the grammar within the requirements of an LL(1) grammar. The consequence of this is that new rules must not introduce ambiguity between productions, and compromises were made to the syntax of the extensions to ensure that minimal changes to existing parsing code were required. The new grammar is backwards compatible with the existing grammar.

The syntax for generic function definitions and for specifying type parameters at call sites is similar to the generic syntax in Java and C\#. Angled brackets (`\textbf{$<$}' and `\textbf{$>$}') were chosen as delimiters for the type parameter sections in function signatures and calls. The other common delimiters (`\textbf{(}', `\textbf{[}' and `\textbf{\{}') are all part of existing language constructs that are valid after identifiers (function calls, index expressions, and composite literals, respectively) so they could not be used without introducing ambiguity or making more extensive changes to the grammar. Angled brackets also have the advantage of producing less confusion for the programmer, as they are not used as delimiters anywhere else in the grammar. The function type signature, which includes the arguments and return type of a function, was extended to include an optional section for type parameters and their type bounds. This allows type parameters to be included in both function definitions and function literals, as well as function types for fields and parameters.

Unfortunately, the `\textbf{$<$}' character cannot be unambiguously parsed as either the start of a list of type arguments or an operator when it occurs after an identifier. For this reason, the type arguments to a function call are supplied as part of the argument list, after the opening parenthesis of the call expression. This syntax is inelegant, but it allows for unambiguous parsing with minimal grammar rewriting, and the type inference features of the language mean that in many cases the entire type parameter section of the call can be omitted and inferred based on the argument types.

One other issue that had to be addressed to allow for these grammar extensions is introduced by the lexer, which translates the source file into a list of tokens for easier parsing. Since `\textbf{$>>$}' is a token used for bit-shifting, the parser must be able to accept `\textbf{$>>$}' or  `\textbf{$>$}' when consuming closing brackets in nested type parameter sections. This is accomplished by tracking whether or not the parser is inside a nested type parameter list, and allowing  `\textbf{$>>$}' to close the inner and outer parameter lists. Using this system, there is no risk of the right shift operator being mistaken for a part of a parameter list, as it is not valid in type signatures, and any combination of single or double angle brackets can be parsed correctly.

\section{Type Checking} \label{type_checking}

Once the new language constructs were correctly parsed into AST nodes, the type checker was updated to respect the new typing rules. The type checking module of the standard library, \texttt{go/types}, implements a type checker that associates each expression and definition in the AST with a type and verifies that all of the type rules are obeyed. The type checker recursively processes each node in the AST, building up types for complex expressions from their constituents. The system also infers types if they are omitted in the short form of variable assignment. 

In generic function definitions, an extra parameter list is included that contains named type parameters and their type bounds:

{ \tt \small
\begin{verbatim}
func foo<A Bound>(x A) A
\end{verbatim}
}

In this example, \texttt{A} is the type parameter, \texttt{Bound} is the type bound of \texttt{A}, and \texttt{foo} has type $\texttt{A} \rightarrow \texttt{A}$. Currently, each type parameter has exactly one type bound, which describes the operations that the type parameter must support. Within the body of the function, the type parameters are assumed to implement the interface of the type bound, but are otherwise unknown types. This means that values of type \texttt{A} support the operations defined on interface \texttt{Bound}, but \texttt{A} and \texttt{Bound} are not considered to be identical types. As a consequence, complex types such as slice\footnote{Slices in Go are dynamic lists of variable size that are backed by arrays} types \texttt{[]A} and \texttt{[]Bound} are not assignable to each other.

At the call sites of a generic function, each type parameter is instantiated with a specific concrete type. Instances of the type parameter are substituted with the specific type argument for that call site, and the function call is type checked using the usual rules. If \texttt{S} is a type that implements \texttt{Bound} and \texttt{val} has type \texttt{S}, then the call expression
{ \tt \small
\begin{verbatim}
foo(<S>, val)
\end{verbatim}
}
uses \texttt{S} as the specific type argument for parameter \texttt{A}. In this case, the type of the complete call expression is \texttt{S}. In functions where the type parameters are used in the formal parameters of the function signature, their types can be inferred from the types of the arguments to the function. In this case, the function call could be written as \texttt{foo(val)} without changing the meaning.

\subsection{Similar Type Systems} \label{similar_type_systems}

This implementation of generic functions is similar to the generic implementations in Java and C\#, with some characteristics of the template system in C++ \cite{ghosh2004generics} \cite{csharp}. In all of these systems, type parameters represent (potentially bounded) unknown runtime types. Differences from the Java and C\# systems emerge due to Go's existing type system. Because there is no concept of inheritance in Go, the only way for type parameters to represent multiple potential runtime types is if they are bounded by interface types. Since type parameters bounded by non-interface types would only have a single valid type substitution, they are disallowed in this system. Another difference from the Java and C\# systems is that Go's interface system is completely structural rather than nominal, i.e., there is no need to indicate which interfaces a particular type implements. Because of this, the proposed generic system in Go is also structural, and is similar to the concept system used in C++ to describe which types are valid parameters to a class template. The type bound for a type parameter describes which operations must be supported on the runtime type, whereas in C++ this is usually specified by a contract in the documentation or through more complex methods \cite{siek2000concept}. The C++ compiler essentially performs structural subtype checking when a class template is instantiated with a concrete type, but because the required operations are not explicitly declared in the type system the communication of these requirements to the programmer is less intuitive unless complex concept checking practices are followed. The generic function system described here gains some of the benefits of the structural type bounding, such as the ability to define the required operations of a type without regard to an existing type hierarchy, while also providing the explicit communication of the nominal systems.

\subsection{Variance} \label{variance}

Go's type system simplifies the implementation of generic functions by removing the need to consider variance in many cases. If a type system contains both parametric polymorphism and subtype polymorphism, additional information must be supplied for each type parameter to indicate how it will interact with the subtype system. Type parameters can take three forms that have different subtype behavior: invariant, covariant, or contravariant. If $<:$ is the subtype relationship, then the following rules apply to the subtype relationship for a type \texttt{T<S>} (\texttt{T} has type parameter \texttt{S}):


\begin{itemize}
\item If \texttt{S} is invariant, there is no additional subtype relationship:
\begin{mathpar}
\inferrule*{ }{$$\texttt{T<A>} <: \texttt{T<A>}$$}
\end{mathpar}
\item If \texttt{S} is covariant:
\begin{mathpar}
\inferrule*{$$\texttt{A} <: \texttt{B}$$}{$$\texttt{T<A>} <: \texttt{T<B>}$$}
\end{mathpar}

\item If \texttt{S} is contravariant:
\begin{mathpar}
\inferrule*{$$\texttt{B} <: \texttt{A}$$}{$$\texttt{T<A>} <: \texttt{T<B>}$$}
\end{mathpar}
\end{itemize}

In Go's type system, the only subtype relationship is the structural subtyping rule for interfaces. Specifically, the interface subtype rule states that a type \texttt{A} implements interface \texttt{I} if the methods in \texttt{I} are defined on \texttt{A}. Since this is the only subtype relationship that exists in Go's type system, any two distinct function types are not subtypes of each other, regardless of the subtype relationships of their arguments or return values. 

If the generic system was extended to include generic structs and interfaces, then variance would need to be considered on the type parameters of interfaces. Additional annotations on the type parameter list of generic interfaces would allow the programmer to indicate the variance of each parameter. The type checker would need to ensure that certain rules about the uses of covariant and contravariant type parameters were followed, to ensure that the subtyping rule implied by the variance of the type parameters would result in only type safe operations.

\section{Code Generation} \label{code_generation}

The most involved changes to the compiler were required to generate the correct code to implement the new language constructs. Generic functions must be able to accept arguments of types that are different than the types used to type check and generate the body of the function. In the case of arguments that have the direct type of a type parameter, the argument can simply be converted to the interface type of the type bound and used as a regular interface within the body of the function. If a return value from the function uses a type parameter directly, a type assertion can be used after the value is returned to transform it back into the specific type substituted for that type parameter at the call site. Additional steps must be taken for arguments that have more complex types containing type parameters, which will be referred to as complex parameterized types. For these types, such as slices of or pointers to a type parameter, there is no way within the existing type system to translate the memory layout of the actual argument to the expected format. For these values, the code generation system must be extended to provide a method of generating the generic function that is independent of the memory layout of the argument values. There are several possible methodologies for accomplishing this, with trade offs in implementation complexity, runtime speed, and code size. 

\subsection{AST Transformation} \label{ast_transformation}

A process of AST transformation is used to translate the input AST into a standard Go AST with no generic functionality. This AST transformation is implemented as a depth-first traversal of the AST, with each node being transformed by a function into its standard form. In this way, complex expressions are translated by first transforming their parts, and then performing any additional modifications needed to combine the simple expressions into the complex one.

For statements, additional bookkeeping must be done to accommodate transformations that turn one statement into several statements. For instance, this is necessary when dealing with functions with multiple return values. Multiple return values are only accessible through a multi-part assignment statement, so any transformations that need to be made to the individual return values must be made in a separate statement. For example, consider a function \texttt{foo} that returns two values, where we would like to apply a function \texttt{f} to each value:
{ \tt \small
\begin{verbatim}
a, b := foo()
\end{verbatim}
}
This must be transformed into the two statements:
{ \tt \small
\begin{verbatim}
a', b' := foo()
a, b := f(a), f(b)
\end{verbatim}
}
where \texttt{a'} and \texttt{b'} represent new temporary variables. The AST transformation accommodates this by allowing the transformation function for statements to return a list of result statements, instead of only a single statement. It is left to the surrounding context of the statement to determine the correct way to insert the additional statements. Within blocks, the new statements are simply added to the body of the block, but there are other contexts for statements that require additional thought. There are instances where only a single statement is allowed, such as parts of \texttt{if} statements and \texttt{for} loops. In these cases, if the statement is transformed into multiple statements, the extra statements must be inserted elsewhere in the AST because only one of the resulting statements can take the place of the original value. For example, consider the case where the previous assignment statement occurred at the beginning of an \texttt{if} statement:
{ \tt \small
\begin{verbatim}
if a, b := foo(); a != b { ... }
\end{verbatim} 
}

This must be transformed as follows:
{ \tt \small
\begin{verbatim}
a', b' := foo()
if a, b := f(a), f(b); a != b { ... }
\end{verbatim}
}

If, instead, the assignment statement occurred as the update statement of a \texttt{for} loop:
{ \tt \small
\begin{verbatim}
for a, b := 0, 0;
 a == b;
 a, b = foo() { ... }
\end{verbatim} 
}
This would be transformed as:
{ \tt \small
\begin{verbatim}
var a', b' T
for a, b := 0, 0;
 a == b;
 a, b = f(a'), f(b') {
  ...
  a', b' = foo()
}
\end{verbatim} 
}

\subsection{Reflection} \label{reflection}

The code generation method used involves rewriting expressions that include values of complex parameterized types into invocations of the runtime introspection system. The Go standard library provides the \texttt{reflect} package, which includes mechanisms for operating on types and values based on their runtime type without knowing the memory layout at compile time. The \texttt{reflect} package includes two main types, \texttt{reflect.Value} and \texttt{reflect.Type}, which provide runtime representations of the value and type sections of interface values. The inputs to the \texttt{reflect} package are values of the empty interface type, which all values conform to. Once a value has been transformed into a \texttt{reflect.Value}, operations that are normally valid on complex types, such as dereferencing a pointer, indexing a slice, or updating a map, are available as operations on the \texttt{reflect.Value}. The results of these reflection expressions can be extracted as an empty interface value, which can optionally be transformed into the concrete expected type using type assertions. In this manner, any value with a complex parameterized type is transformed into an empty interface, and any operations on these values are transformed into reflection operations.

There are many functions in the Go standard library that make use of \begin{em}de-facto\end{em} parametric polymorphism, in that they operate on collection types independent of the element types of the collections. These functions must also be transformed into their reflection equivalents, since all collection values that have type parameters as element types will be represented as empty interface values instead of collection values.

Care must be taken when performing reflection transformations to avoid extracting the value of one reflection operation as an empty interface, only to immediately transform it back into a \texttt{reflect.Value}. This is important not only for efficiency, but also in cases where a reflection expression is on the left hand side of an assignment. In this case, the \texttt{reflect} package allows certain values to be assignable, but only if they are the result of accessing an addressable part of a value, such as an array element, stuct field, or pointer value. It would not be possible to use the reflection expression generated by transforming the left hand side of such an assignment as an addressable value if it was first extracted as an empty interface and then turned back into a \texttt{reflect.Value}. An example of this scenario is the assignment of a value to a slice index:
{ \tt \small
\begin{verbatim}
s[i] = a
\end{verbatim}
}
The naive reflection transformation includes an extra conversion to an empty interface value. When the value is transformed back into a \texttt{reflect.Value} it is no longer addressable, and the \texttt{Set} operation is invalid on it:
{ \tt \small
\begin{verbatim}
temp := reflect.ValueOf(s).Index(i)
  .Interface()
reflect.ValueOf(temp)
  .Set(reflect.ValueOf(value)) //error
\end{verbatim}
}
The correct transformation omits this unnecessary conversion:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Index(i)
  .Set(reflect.ValueOf(value))
\end{verbatim}
}

After this transformation is accomplished, the bodies of the generic functions are valid standard Go code, and code generation can continue as normal. Some examples of expressions and their reflection versions follow, to illustrate how the transformation occurs:

\begin{itemize}
\item
Pointer dereferencing:
{ \tt \small
\begin{verbatim}
*p
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.Indirect(reflect.ValueOf(p))
  .Interface()
\end{verbatim}
}

\item
Slice indexing:
{ \tt \small
\begin{verbatim}
s[i]
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Index(i).Interface()
\end{verbatim}
}

\item
Slice index assignment:
{ \tt \small
\begin{verbatim}
s[i] = value
\end{verbatim}
}
Reflection:


\item
Length calculation:
{ \tt \small
\begin{verbatim}
len(s)
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Len()
\end{verbatim}
}

\end{itemize}


\subsection{Signature Transformation} \label{signature_transformation}

In addition to transforming the bodies of generic functions, their type signatures and calling expressions must also be changed. Any argument or return types that are complex parameterized types are transformed into empty interface values, so that the correct transformations are made to wrap input values in the empty interface context. Call sites of such functions are updated to perform type assertions on their results, so that the results are returned in the format that the calling expression expects. Additional parameters are also added to the signatures of generic functions. The added parameters are \texttt{reflect.Type} values that correspond to specific types substituted for each type parameter at the call site. These values are needed so that allocations involving type parameters can be translated into their reflection versions. For example, a function may include a call to \texttt{make} for a slice of a type parameter \texttt{A}:
{ \tt \small
\begin{verbatim}
func f<A B>() []A {
  return make([]A, 10)
}
\end{verbatim} 
}
This function will be transformed into:
{ \tt \small
\begin{verbatim}
func f<A B>(t$A reflect.Type) interface{} {
  return reflect.MakeSlice(t$A, 10)
}
\end{verbatim} 
}

If the reflection version of \texttt{make} was not used in this case, the returned slice would always be a slice of the type bound of \texttt{A}, not a slice of the concrete type supplied at the call site. A type assertion at the call sites of \texttt{f} transforms the resulting empty interface value into the slice of the concrete type expected.

\subsection{Alternatives for Code Generations} \label{code_gen_alt}

The reflection method of generating generic code is only one implementation possibility. The reflection system makes use of the structure of interface values internally. Each interface value consists of two pointers, one to the data contained within the value, and another to the type information for the runtime concrete type of the value. The reflection system operates on these type and value pointers, performing the necessary transformations to each to produce the next set of values. Using the reflection system directly involves many function calls, as well as runtime checks to ensure that the operations performed on the reflection values are valid given the actual runtime type of the input. In the case of generic code generation, these calls could be avoided by performing the operations on the type and value pointers directly without invoking a library. Many of the runtime checks could potentially be avoided as well, since the type system should ensure that the input values have the correct form before the reflection operations take place. This inline introspection could provide performance improvements over the AST transformations used here. They would require significant changes to the lower level parts of the code generation system, at both the SSA and LLVM IR levels. The current system has the advantage that since it does not change the low level code generation system, it cannot introduce bugs in these complex systems. For a proof of concept, the advantages in ease of development and debugging were chosen over the potential performance gains.

Another code generation possibility would avoid use of introspection completely. The need for introspection arises because the memory layout of values of different instantiations of a generic function are different, and the body of a generic function must be able to handle values with many different memory layouts. One way to avoid this complication is to create distinct versions of each function body for all of the different concrete instantiations of the function that are used in the program. This is more similar to the template system in C++, and involves duplicating generic code to produce non-generic versions that  operate normally. This technique has the performance advantage of allowing static dispatch of methods called on values that would otherwise require dynamic dispatch using the interface system. Statically dispatched methods can more easily be optimized or inlined at the call site. In addition to static dispatch, specialized versions of generic functions can allocate more values on the stack or in registers because the memory layout is fixed and known at compile time, which can expose additional optimizations. 

The drawbacks of specialization are in code size and compilation time. A copy of each generic function must be generated and compiled for every combination of type variables that is used in the program. The existence of function pointers compounds this drawback. When pointers to generic functions are used, it is no longer possible in general to determine the minimal set types that are used as arguments to a function. Pointer analysis can reduce the number of functions that a function pointer can alias, but in the worst case it is possible to have function pointers that can alias any function that matches their signature. This means that any calls to that function pointer produce copies of every function with a matching signature, which may then cause additional functions to be generated if the bodies of those functions themselves call generic functions. Function pointers also require runtime changes to code generation, as any function pointer with a generic signature must now refer to the table of all instantiations of the function that the pointer references. This allows the eventual call site to select the correct instantiation based on the type arguments. 

Since specialization and introspection both have drawbacks, in code size and performance respectively, a hybrid approach may be the most effective. Heuristics or runtime information could be used to determine which functions are called most often so that they could be partially specialized for the most common type arguments. Compiler hints could also be used by the programmer to indicate which functions would likely benefit from specialization. The hybrid approach has the benefit of optimizing the hot paths without introducing significant code bloat. This method could gain the benefits of both code generation methodologies, but it would require all of the implementation work for building both methods and the heuristics needed to choose the generation strategy.

\section{Performance} \label{results}

The use of reflection to implement polymorphism does incur a significant runtime cost. Two benchmarks are presented below, with four implementations compared. The first and second implementations are identical, but the first is compiled using the existing LLGO compiler, while the second is compiled with the modified version. Both versions are included to demonstrate that non-generic code is unaffected by the modifications to the language. The first and second implementations are conventional functions that can only operate on a single type and have no polymorphism. The third implementation uses hand-written reflection operations to provide support for polymorphism, but it does not use the generic system. The final implementation uses the generic system. The two benchmarks represent functions that generate different proportions of their code as reflection operations. 

The first benchmark, in Figure \ref{fig:map}, is an implementation of a \texttt{map} function, which applies a function to each element in a slice in order to produce a new slice. The \texttt{map} function is a staple of functional programming techniques, and it benefits greatly from the type safety introduced by parametric polymorphism. For the benchmark, the function that is used in the \texttt{map} operation is the factorial function, which is used as a placeholder for any function that performs significant computation on the values within the slice. The \texttt{map} function performs relatively little manipulation of complex parameterized types, as each element is retrieved once from one slice and set once in another. The \texttt{map} function was run on lists of $10,000$ elements ranging from 0 to 20. The hand-written reflection implementation was $3.72$ times slower on average than the conventional version. The generic version was $4.13$ times slower on average than the conventional version, but only $11\%$ slower than the hand-written reflection implementation.

\begin{figure}
    \caption{Map Runtime\label{fig:map}}
    \centering
    \input{map.pgf}
\end{figure}

The second benchmark, in Figure \ref{fig:quicksort}, is a naive \texttt{quicksort} function, which sorts an input slice. The \texttt{quicksort} function accepts a predicate as an argument that compares two values, so that collections of arbitrary types may be sorted. The \texttt{quicksort} algorithm is close to a worst case scenario for this generic implementation, because in contrast to \texttt{map}, most of the computational work in the \texttt{quicksort} implementation is directly working with complex parametric types. Each comparison and swap requires index operations to access and set the values within the slice, and each recursive call to \texttt{quicksort} involves creating a new slice using an interval of the input slice. The \texttt{quicksort} function was called with a list of $10,000$ random integers, and a list of $10,000$ random floating point values. The reflection implementation was $18.88$ times slower than the conventional version. The generic implementation was $27.59$ times slower than the conventional version, and $1.46$ times slower than the hand-written reflection implementation.

\begin{figure}
    \caption{Quicksort Runtime\label{fig:quicksort}}
    \centering
    \input{quicksort.pgf}
\end{figure}

Both reflection implementations have additional overhead compared to the conventional version due to the allocations that must be performed to create the intermediate reflection values and operate on them. The hand-written reflection implementation can reduce duplication of reflection operations by reusing values in multiple expressions. Profiling data for the hand-written reflection implementation\footnote{Profiling is not currently available for generic functions} in Table \ref{tab:reflect_profile} indicates that most of the additional execution time is spent within the allocation code in the \texttt{reflect} package, particularly when extracting values out of the reflection context after an operation completes.  The conventional implementation also has the advantage of allowing many operations to be statically allocated, or to use registers. More optimization opportunities are exposed to the compiler in the conventional versions for this reason.

\begin{table}
\centering
% BEGIN RECEIVE ORGTBL sort_reflect_profile
\begin{tabular}{lrl}
Time & Time (\%{}) & Function \\
\hline
8.83s & 76.92\%{} & main.quicksortReflect \\
5.09s & 44.34\%{} & reflect.Value.Interface \\
4.80s & 41.81\%{} & reflect.valueInterface \\
4.38s & 38.15\%{} & reflect.packEface \\
3.07s & 26.74\%{} & runtime.newobject \\
2.91s & 25.35\%{} & reflect.unsafe\_{}New \\
2.73s & 23.78\%{} & runtime.mallocgc \\
1.18s & 10.28\%{} & runtime.typedmemmove \\
1.03s & 8.97\%{} & reflect.typedmemmove \\
0.85s & 7.40\%{} & reflect.Value.Set \\
0.80s & 6.97\%{} & runtime.memmove \\
0.62s & 5.40\%{} & runtime.assertE2T \\
0.59s & 5.14\%{} & reflect.Value.Index \\
\end{tabular}
% END RECEIVE ORGTBL sort_reflect_profile
\begin{comment}
#+ORGTBL: SEND sort_reflect_profile orgtbl-to-latex :splice nil :skip 0
| Time  | Time (%) | Function                |
|-------+----------+-------------------------|
| 8.83s |   76.92% | main.quicksortReflect   |
| 5.09s |   44.34% | reflect.Value.Interface |
| 4.80s |   41.81% | reflect.valueInterface  |
| 4.38s |   38.15% | reflect.packEface       |
| 3.07s |   26.74% | runtime.newobject       |
| 2.91s |   25.35% | reflect.unsafe_New      |
| 2.73s |   23.78% | runtime.mallocgc        |
| 1.18s |   10.28% | runtime.typedmemmove    |
| 1.03s |    8.97% | reflect.typedmemmove    |
| 0.85s |    7.40% | reflect.Value.Set       |
| 0.80s |    6.97% | runtime.memmove         |
| 0.62s |    5.40% | runtime.assertE2T       |
| 0.59s |    5.14% | reflect.Value.Index     |
\end{comment}
\caption{Cumulative runtime of functions in the hand-written implementation of \texttt{quicksort}. Only functions called from within the sorting code and that account for at least 5\% of the total runtime are included. The \texttt{runtime/pprof} package of the Go standard library was used to collect performance data.}\label{tab:reflect_profile}
\end{table}

These results indicate that the runtime cost associated with generic functions is significant in this implementation. Performing the introspection directly instead of dispatching to the existing reflection system could produce performance gains by avoiding the allocation of intermediate objects. These results also suggest that generating specialized versions of generic function implementations could provide significant performance improvements, as the specialized functions generated would be closer to the hand-written conventional functions that served as the baseline for these benchmarks. Since the performance of non-generic code is unaffected by the generic system, the programmer can determine if the abstraction provided by generic programming is worth the trade off in speed.

\section{Conclusion and Future Work} \label{conclusion}

The generic function implementation described demonstrates that parametric polymorphism is a concept that is compatible with Go. Two of Go's core philosophies are simplicity and ease of use, and the extensions to the language required to add generics do not violate these principles. The syntax extensions are not substantially harder to understand or use than existing language constructs, and the type system is still less complex than comparable languages like C++ or Java due to the lack of inheritance. The additional complexity introduced in the type system is balanced by a reduction in complexity in user code and increased expressiveness for writing reusable programs. This implementation provides a proof of concept for how a generic type system might interface with existing Go language constructs, and demonstrates that the language can be extended in a backwards compatible way.

An extension to the generic type system would be to allow for multiple type bounds for each type parameter. This can be emulated for non-overlapping interface type bounds in the current implementation by constructing a new interface type that has all of the desired type bounds as embedded interfaces. If the type bounds have overlapping method sets, a new interface must be constructed that contains the union of all of the method sets of the type bounds. In the future, this step could be performed automatically for all supplied type bounds. 

The runtime costs incurred by this implementation of generic functions are significant, and do challenge Go's focus on performance. Several techniques for improving the performance of generic code have been discussed, and future work can evaluate if one of these proposals provides satisfactory performance trade offs. The changes needed to evaluate the performance of function specialization or inline introspection are limited to the code generation section of the compiler, and the existing infrastructure for parsing and type checking the generic language constructs can be reused. The performance penalties of this particular implementation of the generic function system do not necessarily indicate that the language extension is incompatible with a performance oriented language, only that additional work is needed to satisfy this design goal.

A complete implementation of parametric polymorphism will need to contain mechanisms for generic interfaces and structs. Generic functions are a necessary first step to implementing these other generic constructs, as both interfaces and structs are largely defined by their member functions. The principles used in parsing and type checking generic functions are extendable to the other language constructs, and some parts of the implementation may be reused.

Finally, if a complete, performant generic type implementation is developed it will need to be implemented in the major Go compilers, GC and GCCGo. The LLGO compiler is useful in developing the prototype system and for more rapid development of alternative implementations for performance comparisons, but the main Go compilers are what define the language in practice.

The source code for the modified version of the LLGO compiler can be found at \url{https://github.com/Matt343/llgo}.

{\footnotesize \bibliographystyle{acm}
  \bibliography{thesis}

}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
