% TEMPLATE for Usenix papers, specifically to meet requirements of
% USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
% written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
% Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
% both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

\documentclass[letterpaper,twocolumn,11pt]{article}
\usepackage{epsfig,endnotes,amsmath,pgf}
\usepackage[toc,page]{appendix}
\begin{document}

% don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Parametric Polymorphism in the Go Programming Language}

\author{
  {\rm Matthew Allen}\\
  University of Texas
  \and
  {\rm Jan Rellermeyer}\\
  Second Institution
}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}
Your Abstract Text Goes Here.  Just a few facts.
Whet our appetites.

\section{Introduction} \label{introduction}

Strong, static type systems offer the programmer the ability to reason about the kinds of data that a program operates on and ensure that only valid data is used to perform calculations. When using a language with a strong, static type system, there are certain guarantees of correctness that are offered, and certain classes of errors that can be detected at compile time rather than at run time. As with any form of static analysis, compile time type checking cannot completely understand the semantics of the program, and so it must be conservative in rejecting potentially valid programs to ensure that type errors cannot occur. If a language implements an overly simplistic static type system, the restrictions of the type system prevent the programmer from writing clear, expressive, and reusable code. In this case, the drawbacks of a strong type system may outweigh the benefits. By adding more power to the type system, the language can become more expressive and the type system can be less of a burden on the programmer. Parametric polymorphism is a common feature of strong type systems that allows functions or data structures to be written generically, so that they can operate on values of many types while still maintaining static type safety. Generic functions and data structures have type parameters which are substituted for specific types when the function or data structure is used. These type parameters may be bounded, so that only a certain class of types can be used for a type parameter, which allows the generic construct to use some common features of its parameterized types.

\section{Motivation} \label{motivation}

The Go programming language features a strong static type system, but the semantics of the type system are quite limited. Without a way to use parametric polymorphism, it is impossible to write statically type checked functions that can be reused for multiple similar types. This leads programmers to either duplicate code many times to create separate, specialized versions of otherwise identical functions, or to bypass the static type system using casting and reflection. Both of these approaches have practical drawbacks compared to generic programming, and they introduce increased potential for errors. In this paper, I outline an extension to the Go language that includes generic functions that are statically checked and transformed into standard Go code at compile time. 

Since Go is a compile language, changes to the language specification must be accomplished by changing the compiler(s) that implement the language. The new language constructs necessary to implement generic functions require changes to the grammar that defines valid Go programs. Because of this, the parser for the language must be updated, and new node types must be added to the abstract syntax tree (AST) of the language. These changes to the parser are outlined in section \ref{parsing}. Once the compiler can parse the new language constructs, the type checking system must be updated with the new type rules. These changes to the type checking system in the compiler are detailed in section \ref{type_checking}. Section \ref{code_generation} explains the changes needed to generate generic code correctly, as well as the trade offs for different code generation strategies. 

\section{Compiler Architecture} \label{architecture}

There are several compiler implementations for the Go language, each with different advantages and drawbacks. The main compiler is referred to as the Go Compiler, or GC, and is the original version of the compiler. Originally written in C, GC was made self-hosting in Go version 1.5, and it is now written almost entirely in Go. Unfortunately, the process for transitioning GC from C to Go was mostly automated, and as a result the codebase has not truly made use of the features or advantages that Go has over C. One important consequence of this is that the compiler does not use the parsing and type checking libraries for Go code that are included in the standard library. The code for performing these tasks is relatively tightly coupled with other parts of the system. For this reason, adding new features to the GC system is difficult, and a significant amount of work would have to be dedicated to working around the complexities of the C-like software design.

 Another major compiler implementation is GCCGo, which is a Go frontend for the GNU Compiler Collection (GCC). GCCGo is written in C++ in the style of the GCC codebase. Since GCCGo is written in C++, it also does not make use of the parsing and type checking libraries from the Go standard library. As a GCC frontend, the compiler inherits much of the complexity that is required of such a flexible compiler, and modifications to it must account for this overhead.

I chose to modify a less commonly used community compiler, LLGO, which is a frontend for the LLVM compiler system. LLGO is written in Go, and since it uses the standard library parsing and type checking systems it was the easiest system to add modifications to and extend. Since the goal of this project was to provide a proof of concept for a generic system in Go, I chose to sacrifice widespread usage in favor of ease of development and prototyping speed.

The LLGO compiler consists of an implementation of the Go runtime, and a frontend compiler that emits LLVM intermediate representation (IR). The main program emulates the command line API of GC and GCCGo, providing support for resolving dependencies, building packages, and linking programs. When a program is processed by the compiler, it first invokes the parser on the input files to generate a canonical AST for the program. Next, the type checker is invoked on the AST, which generates type information for each expression and definition and ensures that all of the rules of the type system are followed. Once the type information has been generated, the AST is transformed into a single static assignment (SSA) form, which makes the AST more compatible with the SSA form of the LLVM IR. At this stage the LLVM specific translation occurs, generating IR from the SSA form of the program that can be fed into the LLVM system. LLVM includes many compiler passes that perform optimization and improve the final code generation, and several of these passes are used to implement the compiler optimization for LLGO. Finally, the LLVM system outputs an executable, binary version of the input program. This architecture provides a reasonable level of modularity and separation of the various parts of the compilation process. The modularity of the system makes it much easier to implement new features in discrete parts, and provide ways to test the individual parts of the system in isolation.

\section{Parsing} \label{parsing}

The first change that was required to implement generic functions was to extend the parser to support the new language constructs. The Go standard library contains a module, \texttt{go/parser}, that implements a recursive descent parser for the Go language grammar. The language grammar was designed to be an LL(1) grammar, which means it can be parsed by a recursive descent parser with single element lookahead and no backtracking in linear time. The parser, therefore, is implemented without support for backtracking. To avoid an extensive redesign of the existing parser, the extensions to the language were constrained to keep the grammar within the requirements of an LL(1) grammar. The practice consequence of this is that new rules must not introduce ambiguity between productions. Compromises were made to the syntax of the extensions to ensure that minimal changes to existing parsing code were required, and the new grammar is backwards compatible with the existing grammar.

The syntax for generic function definitions and for specifying type parameters at call sites is similar to the generic syntax in Java and C$\sharp$. Angled brackets (`\textbf{$<$}' and `\textbf{$>$}') were chosen as delimiters for the type parameter sections in function signatures and calls. The other common delimiters (`\textbf{(}', `\textbf{[}' and `\textbf{\{}') are all part of existing language constructs that are valid after identifiers (function calls, index expressions, and composite literals, respectively) so they could not be used without introducing ambiguity or making more extensive changes to the grammar. Angled brackets also have the advantage of producing less confusion for the programmer, as they are not used as delimiters anywhere else in the grammar. The function type signature, which includes the arguments and return type of a function, was extended to include an optional section for type parameters and their type bounds. This allows type parameters to be included in both function definitions and function literals, as well as function types for fields and parameters. Unfortunately, the `\textbf{$<$}' character cannot be unambiguously parsed as either the start of a list of type arguments or the less than operator when it occurs after an identifier. For this reason, the type arguments to a function call are supplied as part of the argument list, after the opening parenthesis of the call expression. This syntax is inelegant, but it allows for unambiguous parsing with minimal grammar rewriting, and the type inference features of the language mean that in many cases the entire type parameter section of the call can be omitted and inferred based on the argument types.

One other issue that had to be addressed to allow for these grammar extensions is introduced by the lexer, which translates the source file into a list of tokens for easier parsing. Since `\textbf{$>>$}' is a token used for bit-shifting, the parser must be able to accept `\textbf{$>>$}' or  `\textbf{$>$}' when consuming closing brackets in nested type parameter sections. This is accomplished by tracking whether or not the parser is inside a nested type parameter list, and allowing  `\textbf{$>>$}' to close the inner and outer parameter lists. Using this system, there is no risk of the right shift operator being mistaken for a part of a parameter list, as it is not valid in type signatures, and any combination of single or double angle brackets can be parsed correctly.

The complete changes to the Go grammar are included in Appendix \ref{App:grammar}.

\section{Type Checking} \label{type_checking}

Once the new language constructs were correctly parsed into AST nodes, the type checker was updated to respect the new typing rules. The type checking module of the standard library, \texttt{go/types}, implements a type checker that associates each expression and definition in the AST with a type and verifies that all of the type rules are obeyed. The type checker recursively processes each node in the AST, building up types for complex expressions from their constituents. The system also infers types if they are omitted in the short form of variable assignment. 

In generic function definitions, an extra parameter list is included that contains named type parameters and their type bounds:

{ \tt \small
\begin{verbatim}
func foo<A B>(x A) A
\end{verbatim}
}

In this example, \texttt{A} is the type parameter, \texttt{B} is the type bound of \texttt{A}, and \texttt{foo} has type $\texttt{A} \rightarrow \texttt{A}$. The type bound must be an interface type, or a named type that represents an interface type, because these are the only types in Go's type system that can have nontrivial subtypes. Within the body of the function, the type parameters are assumed to implement the interface of the type bound, but are otherwise unknown types. This means that values of type \texttt{A} support the operations defined on interface \texttt{B}, but \texttt{A} and \texttt{B} are not considered to be identical types. As a consequence, complex types such as slice types \texttt{[]A} and \texttt{[]B} are not assignable to each other.

At the call sites of a generic function, each type parameter is instantiated with a specific concrete type. Instances of the type parameter are substituted with the specific type argument for that call site, and the function call is type checked using the usual rules. If \texttt{S} is a type that implements \texttt{B} and \texttt{val} has type \texttt{S}, then the call expression
{ \tt \small
\begin{verbatim}
foo(<S>, val)
\end{verbatim}
}
uses \texttt{S} as the specific type argument for parameter \texttt{A}. In this case, the type of the complete call expression is \texttt{S}. In functions where the type parameters are used in the formal parameters of the function signature, their types can be inferred from the types of the arguments to the function. In this case, the function call could be written as \texttt{foo(val)} without changing the meaning.

\subsection{Variance} \label{variance}

Go's type system simplifies the implementation of generic functions by removing the need to consider variance in many cases. If a type system contains both parametric polymorphism and subtype polymorphism additional information must be supplied for each type parameter to indicate how it will interact with the subtype system. Type parameters can take three forms that have different subtype behavior: invariant, covariant, or contravariant. If $<:$ is the subtype relationship, then the following rules apply to the subtype relationship for a type \texttt{T<S>} (\texttt{T} has type parameter \texttt{S}):

\begin{itemize}
\item If \texttt{S} is invariant: \\
\[\texttt{A} = \texttt{B} \iff \texttt{T<A>} <: \texttt{T<B>}\]
\item If \texttt{S} is covariant:\\
\[\texttt{A} <: \texttt{B} \iff \texttt{T<A>} <: \texttt{T<B>}\]
\item If \texttt{S} is contravariant:\\
\[\texttt{B} <: \texttt{A} \iff \texttt{T<A>} <: \texttt{T<B>}\]
\end{itemize}

In Go's type system, the only subtype relationship is the structural subtyping rule for interfaces. Specifically, the interface subtype rule for a type \texttt{A} and interface \texttt{I} is:\\
\[\texttt{A}\textrm{ implements the methods in }\texttt{I} \iff \texttt{A} <: \texttt{I}\]\\
Since this is the only subtype relationship that exists in Go's type system, any two distinct function types are not subtypes of each other, regardless of the subtype relationships of their arguments or return values. 

If the generic system was extended to include generic structs and interfaces, then variance would need to be considered on the type parameters of interfaces. Additional annotations on the type parameter list of generic interfaces would allow the programmer to indicate the variance of each parameter. The type checker would need to ensure that certain rules about the uses of covariant and contravariant type parameters were followed, to ensure that the subtyping rule implied by the variance of the type parameters would result in only type safe operations.

\section{Code Generation} \label{code_generation}

The most involved changes to the compiler were required to generate the correct code to implement the new language constructs. Generic functions must be able to accept arguments of types that are different than the types used to type check and generate the body of the function. In the case of arguments that have the direct type of a type parameter, the argument can simply be converted to the interface type of the type bound and used as a regular interface within the body of the function. If a return value from the function uses a type parameter directly, a type assertion can be used after the value is returned to transform it back into the specific type substituted for that type parameter at the call site. Additional steps must be taken for arguments that have more complex types containing type parameters, which I will refer to as complex parameterized types. For these types, such as slices of or pointers to a type parameter, there is no way within the existing type system to translate the memory layout of the actual argument to the expected format. For these values, the code generation system must be extended to provide a method of generating the generic function that is independent of the memory layout of the argument values. There are several possible methodologies for accomplishing this, with trade offs in implementation complexity, runtime speed, and code size. 

\subsection{AST Transformation} \label{ast_transformation}

A process of AST transformation is used to translate the input AST into a standard Go AST with no generic functionality. This AST transformation is implemented as a depth-first traversal of the AST, with each node being transformed by a function into its standard form. In this way, complex expressions are translated by first transforming their parts, and then performing any additional modifications needed to combine the simple expressions into the complex one.

For statements, additional bookkeeping must be done to accommodate transformations that turn one statement into several statements. For instance, this is necessary when dealing with functions with multiple return values. Multiple return values are only accessible through a multi-part assignment statement, so any transformations that need to be made to the individual return values must be made in a separate statement. For example, consider a function \texttt{foo} that returns two values, where we would like to apply a function \texttt{f} to each value:
{ \tt \small
\begin{verbatim}
a, b := foo()
\end{verbatim}
}
This must be transformed into the two statements:
{ \tt \small
\begin{verbatim}
a', b' := foo()
a, b := f(a), f(b)
\end{verbatim}
}
where \texttt{a'} and \texttt{b'} represent new temporary variables. The AST transformation accommodates this by allowing the transformation function for statements to return a list of result statements, instead of only a single statement. It is left to the surrounding context of the statement to determine the correct way to insert the additional statements. Within blocks, the new statements are simply added to the body of the block, but there are other contexts for statements that require additional thought. There are instances where only a single statement is allowed, such as parts of \texttt{if} statements and \texttt{for} loops. In these cases, if the statement is transformed into multiple statements, the extra statements must be inserted elsewhere in the AST because only one of the resulting statements can take the place of the original value. For example, consider the case where the previous assignment statement occurred at the beginning of an \texttt{if} statement:
{ \tt \small
\begin{verbatim}
if a, b := foo(); a != b { ... }
\end{verbatim} 
}

This must be transformed as follows:
{ \tt \small
\begin{verbatim}
a', b' := foo()
if a, b := f(a), f(b); a != b { ... }
\end{verbatim}
}

If, instead, the assignment statement occurred as the update statement of a \texttt{for} loop:
{ \tt \small
\begin{verbatim}
for a, b := 0, 0;
 a == b;
 a, b = foo() { ... }
\end{verbatim} 
}
This would be transformed as:
{ \tt \small
\begin{verbatim}
var a', b' T
for a, b := 0, 0;
 a == b;
 a, b = f(a'), f(b') {
  ...
  a', b' = foo()
}
\end{verbatim} 
}

adflaksjflksja**********

\subsection{Reflection} \label{reflection}

The code generation method I used involves rewriting expressions that use values of complex parameterized types into invocations of the runtime introspection system. The Go standard library provides the \texttt{reflect} package, which includes mechanisms for operating on types and values based on their runtime type without knowing the memory layout at compile time. The \texttt{reflect} package includes two main types, \texttt{reflect.Value} and \texttt{reflect.Type}, which provide runtime representations of the value and type sections of interface values. The inputs to the \texttt{reflect} package are values of the empty interface type, which all values conform to. Once a value has been transformed into a \texttt{reflect.Value}, operations that are normally valid on complex types, such as dereferencing a pointer, indexing a slice, or updating a map, are available as operations on the \texttt{reflect.Value}. The results of these reflection expressions can be extracted as an empty interface value, which can optionally be transformed into the concrete expected type using type assertions. In this manner, any value with a complex parameterized type is transformed into an empty interface, and any operations on these values are transformed into reflection operations.

There are many functions in the Go standard library that use a kind of \begin{em}de-facto\end{em} parametric polymorphism, in that they operate on collection types independent of the element types of the collections. These functions must also be transformed into their reflection equivalents, since all collection values that have type parameters as element types will be represented as empty interface values instead of collection values.

Care must be taken when performing reflection transformations to avoid extracting the value of one reflection operation as an empty interface, only to immediately transform it back into a \texttt{reflect.Value}. This is important not only for efficiency, but also in cases where a reflection expression is on the left hand side of an assignment. In this case, the \texttt{reflect} package allows certain values to be assignable, but only if they are the result of accessing an addressable part of a value, such as an array element, stuct field, or pointer value. It would not be possible to use the reflection expression generated by transforming the left hand side of such an assignment as an addressable value if it was first extracted as an empty interface and then turned back into a \texttt{reflect.Value}.

After this transformation is accomplished, the bodies of the generic functions are valid standard Go code, and code generation can continue as normal. Some examples of expressions and their reflection versions follow, to illustrate how the transformation occurs:

\begin{itemize}
\item
Pointer dereferencing:
{ \tt \small
\begin{verbatim}
*p
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.Indirect(reflect.ValueOf(p))
  .Interface()
\end{verbatim}
}

\item
Slice indexing:
{ \tt \small
\begin{verbatim}
s[i]
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Index(i).Interface()
\end{verbatim}
}

\item
Slice index assignment:
{ \tt \small
\begin{verbatim}
s[i] = value
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Index(i)
  .Set(reflect.ValueOf(value))
\end{verbatim}
}

\item
Length calculation:
{ \tt \small
\begin{verbatim}
len(s)
\end{verbatim}
}
Reflection:
{ \tt \small
\begin{verbatim}
reflect.ValueOf(s).Len()
\end{verbatim}
}

\end{itemize}


\subsection{Signature Transformation} \label{signature_transformation}

In addition to transforming the bodies of generic functions, their type signatures and calling expressions must also be changed. Any argument or return types that are complex parameterized types are transformed into empty interface values, so that the correct transformations are made to wrap input values in the empty interface context. Call sites of such functions are updated to perform type assertions on their results, so that the results are returned in the format that the calling expression expects. Additional parameters are also added to the signatures of generic functions. The added parameters are \texttt{reflect.Type} values that correspond to specific types substituted for each type parameter at the call site. These values are needed so that allocations involving type parameters can be translated into their reflection versions. For example, a function may include a call to \texttt{make} for a slice of a type parameter \texttt{A}:
{ \tt \small
\begin{verbatim}
func f<A B>() []A {
  return make([]A, 10)
}
\end{verbatim} 
}
This function will be transformed into:
{ \tt \small
\begin{verbatim}
func f<A B>(t$A reflect.Type) interface{} {
  return reflect.MakeSlice(t$A, 10)
}
\end{verbatim} 
}

If the reflection version of \texttt{make} was not used in this case, the returned slice would always be a slice of the type bound of \texttt{A}, not a slice of the concrete type supplied at the call site. A type assertion at the call sites of \texttt{f} transforms the resulting empty interface value into the slice of the concrete type expected.

\subsection{Alternatives for Code Generations} \label{code_gen_alt}

The reflection method of generating generic code is only one implementation possibility. The reflection system makes use of the structure of interface values internally. Each interface value consists of two pointers, one to the data contained within the value, and another to the type information for the runtime concrete type of the value. The reflection system operates on these type and value pointers, performing the necessary transformations to each to produce the next set of values. Using the reflection system directly involves many function calls, as well as runtime checks to ensure that the operations performed on the reflection values are valid given the actual runtime type of the input. In the case of generic code generation, these calls could be avoided by performing the operations on the type and value pointers directly without invoking a library. Many of the runtime checks could potentially be avoided as well, since the type system should ensure that the input values have the correct form before the reflection operations take place. This inline introspection could provide performance improvements over the AST transformations used here. They would require significant changes to the lower level parts of the code generation system, at both the SSA and LLVM IR levels. The current system has the advantage that since it does not change the low level code generation system, it cannot introduce bugs in these complex systems. For a proof of concept, the advantages in ease of development and debugging were chosen over the potential performance gains.

Another code generation possibility would avoid use of introspection completely. The need for introspection arises because the memory layout of values of different instantiations of a generic function are different, and the body of a generic function must be able to handle values with many different memory layouts. One way to avoid this complication is to create distinct versions of each function body for all of the different concrete instantiations of the function that are used in the program. This is more similar to the template system in C++, and involves duplicating generic code to produce non-generic versions that  operate normally. This technique has the performance advantage of often allowing static dispatch of methods called on values that would otherwise require dynamic dispatch using the interface system. Statically dispatched methods can more easily be optimized or inlined at the call site. In addition to static dispatch, specialized versions of generic functions can allocate more values on the stack or in registers because the memory layout is fixed and known at compile time, which can expose additional optimizations. 

The drawbacks of specialization are in code size and compilation time. A copy of each generic function must be generated and compiled for every combination of type variables that is used in the program. The problem is made worse by the existence of function pointers. When pointers to generic functions are used, it becomes less clear which types are used as arguments to which functions. Pointer analysis can reduce the number of functions that each pointer can possibly refer to, but in the worst case it is possible to have function pointers that can point to any function that matches their signature. This means that any calls to that function pointer produce copies of every function with a matching signature, which may then cause additional functions to be generated if the bodies of those functions themselves call generic functions. Function pointers also require runtime changes to code generation, as any function pointer with a generic signature must now refer to the table of all instantiations of the function that the pointer references. This allows the eventual call site to select the correct instantiation based on the type arguments. Specialization also makes dynamic linking difficult, as the type arguments supplied to each function may not be known until link time. This requires either forbidding dynamic linking, providing enough information to generate the additional specialized functions at link time, or providing non-specialized introspective versions of generic functions for use from outside code. 

Since specialization and introspection both have drawbacks, in code size and performance respectively, a hybrid approach may be the most effective. Heuristics or runtime information could be used to determine which functions are called most often so that they could be partially specialized for the most common type arguments. Compiler hints could also be used by the programmer to indicate which functions would likely benefit from specialization. The hybrid approach has the benefit of optimizing the hot paths without introducing significant code bloat. This method could gain the benefits of both code generation methodologies, but it would require all of the implementation work for building both methods and the heuristics needed to choose the generation strategy.

\section{Results} \label{results}

The generic function implementation detailed here provides a relatively complete extension to the Go language. The implementation has bugs and edge cases that need to be addressed before it can be considered a complete generic function implementation, and significant testing work is required to ensure that the generic code behaves as expected in all circumstances. As a proof of concept, it demonstrates that all of the major language features of Go can be utilized in generic functions, and that the existing reflection capabilities of the language are powerful enough to implement parametric polymorphism.

The use of reflection to implement polymorphism does incur a significant runtime cost. Two benchmarks are presented below, with three implementations compared. The first implementation is a completely specialized function that can only operate on a single type and has no polymorphism. The second implementation uses reflection to provide support for polymorphism, but it does not use the generic system. The final implementation uses the generic system. The two benchmarks represent functions that generate different proportions of their code as reflection operations. 

The first benchmark, in Figure \ref{fig:map} is an implementation of a \texttt{map} function, which applies a function to each element in a slice in order to produce a new slice. The \texttt{map} function is a staple of functional programming techniques, and it benefits greatly from the type safety introduced by parametric polymorphism. For the benchmark, the function that is used int he \texttt{map} operation is the factorial function, which is used as a placeholder for any function that performs significant computation on the values within the slice. The \texttt{map} function performs relatively little manipulation of complex parameterized types, as each element is retrieved from one slice and set in another. The \texttt{map} function was run on lists of $10,000$ elements ranging from 0 to 20. The hand written reflection implementation was about $3.72$ times slower than the specialized version. The generic version was about $4.13$ times slower than the specialized version, but only $11\%$ slower than the hand written reflection implementation.

\begin{figure}
    \caption{Map Runtime\label{fig:map}}
    \centering
    \input{map.pgf}
\end{figure}

The second benchmark, in Figure \ref{fig:quicksort} is a naive \texttt{quicksort} function, which sorts an input slice. The \texttt{quicksort} function accepts a predicate as an argument that compares two values, so that collections of arbitrary types may be sorted. The \texttt{quicksort} algorithm is close to a worst case scenario for this generic implementation, because in contrast to \texttt{map}, most of the computational work in the \texttt{quicksort} implementation is directly working with complex parametric types. Each comparison and swap requires index operations to access and set the values within the slice, and each recursive call to \texttt{quicksort} involves creating a new slice using an interval of the input slice. The \texttt{quicksort} function was called with a list of $10,000$ random integers, and a list of $10,000$ random floating point values. The reflection implementation was $18.88$ times slower than the specialized version. The generic implementation was $27.59$ times slower than the specialized version, and $1.46$ times slower than the hand written reflection implementation.

\begin{figure}
    \caption{Quicksort Runtime\label{fig:quicksort}}
    \centering
    \input{quicksort.pgf}
\end{figure}

Both reflection implementations have additional overhead compared to the specialized version due to the allocations and garbage collection that must be performed to create the intermediate reflection values and operate on them. The hand written reflection implementation can reduce duplication of reflection operations by reusing values in multiple expressions. The specialized implementation also has the advantage of allowing for many operations to be statically allocated, or to use registers. More optimization opportunities are exposed to the compiler in the specialized versions.

These results indicate that the runtime cost associated with generic functions is significant in this implementation. It is not clear what performance gains could be achieved by performing the introspection directly instead of dispatching to the existing reflection system. These results do suggest that generating specialized versions of generic function implementations could provide significant performance improvements, since the specialized functions generated would be closer to the hand written specialized functions that served as the baseline for these benchmarks.

\section{Conclusion and Future Work} \label{conclusion}



{\footnotesize \bibliographystyle{acm}
  \bibliography{sample}}

\theendnotes


\begin{appendices}

\section{Modified Grammar} \label{App:grammar}

\end{appendices}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
